{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgjMeodBbTc9"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as jr\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import optax\n",
        "import orbax.checkpoint as ocp\n",
        "from dynamax.hidden_markov_model import CategoricalHMM\n",
        "from flax import nnx\n",
        "from tqdm import trange\n",
        "\n",
        "# The notebooks now exist in /notebooks and scripts are in /scripts\n",
        "# This allows the scripts to be imported without creating a package for them\n",
        "sys.path.append(str(Path(\".\").resolve().parent / \"scripts\"))\n",
        "import rnnlib\n",
        "import transformerlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v196N4IUnZAO",
        "outputId": "7be68842-8132-4a9d-d0ce-d27eb6c8d276"
      },
      "outputs": [],
      "source": [
        "initial_probs = jnp.array([0.5, 0.5])\n",
        "transition_matrix = jnp.array([[0.95, 0.05], [0.05, 0.95]])\n",
        "emission_probs = jnp.array(\n",
        "    [\n",
        "        [1 / 6, 1 / 6, 1 / 6, 1 / 6, 1 / 6, 1 / 6],  # fair die\n",
        "        [1 / 10, 1 / 10, 1 / 10, 1 / 10, 1 / 10, 5 / 10],  # loaded die\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(f\"A.shape: {transition_matrix.shape}\")\n",
        "print(f\"B.shape: {emission_probs.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4wnSdM_bv4L"
      },
      "outputs": [],
      "source": [
        "num_timesteps = 512  # Simulation length\n",
        "num_states = 2  # two types of dice (fair and loaded)\n",
        "num_emissions = 1  # only one die is rolled at a time\n",
        "num_classes = 6  # each die has six faces\n",
        "\n",
        "# Construct the HMM\n",
        "hmm = CategoricalHMM(num_states, num_emissions, num_classes)\n",
        "\n",
        "# Initialize the parameters struct with known values\n",
        "hmm_params, _ = hmm.initialize(\n",
        "    initial_probs=initial_probs,\n",
        "    transition_matrix=transition_matrix,\n",
        "    emission_probs=emission_probs.reshape(num_states, num_emissions, num_classes),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7rC3lrrvkgX"
      },
      "outputs": [],
      "source": [
        "# == Generate training data == #\n",
        "emissions = []\n",
        "num_iterations = 1000\n",
        "num_train_samples = 5000\n",
        "num_test_samples = 100\n",
        "num_timesteps = 512\n",
        "batch_size = 32\n",
        "\n",
        "train_states, train_emissions = jax.vmap(hmm.sample, in_axes=[None, 0, None])(\n",
        "    hmm_params, jr.split(jr.PRNGKey(0), num_train_samples), num_timesteps\n",
        ")\n",
        "train_emissions = train_emissions[:, :, 0]\n",
        "\n",
        "test_states, test_emissions = jax.vmap(hmm.sample, in_axes=[None, 0, None])(\n",
        "    hmm_params, jr.split(jr.PRNGKey(1), num_test_samples), num_timesteps\n",
        ")\n",
        "test_emissions = test_emissions[:, :, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aXqc-Y6mV4-"
      },
      "outputs": [],
      "source": [
        "# == Create Transformer Model == #\n",
        "transformer_model = transformerlib.TransformerLM(\n",
        "    vocab_size=6,\n",
        "    d_embedding=64,\n",
        "    max_seq_len=512,\n",
        "    num_layers=1,\n",
        "    d_model=64,\n",
        "    num_heads=1,\n",
        "    d_feedforward=128,\n",
        "    attn_dropout_p=0.0,\n",
        "    rngs=nnx.Rngs(params=jr.key(0), dropout=jr.key(1)),\n",
        ")\n",
        "\n",
        "# == Create Optimizer == #\n",
        "learning_rate = 0.01\n",
        "momentum = 0.9\n",
        "transformer_optimizer = nnx.Optimizer(\n",
        "    transformer_model, optax.adamw(learning_rate, momentum)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# == Create Recurrent Model == #\n",
        "recurrent_model = rnnlib.LRUModel(\n",
        "    dim_in=6, dim_hidden=64, dim_out=6, mlp_depth=3, num_layers=3, rngs=nnx.Rngs(0)\n",
        ")\n",
        "\n",
        "# == Create Optimizer == #\n",
        "learning_rate = 0.005\n",
        "momentum = 0.9\n",
        "recurrent_optimizer = nnx.Optimizer(\n",
        "    recurrent_model, optax.adamw(learning_rate, momentum)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bvFZkzDmmPO"
      },
      "outputs": [],
      "source": [
        "# == Method to Sample Training/Test Data == #\n",
        "def sample_emissions(sample_func, seed, batch_size, num_timesteps):\n",
        "    \"\"\"Sample a batch of data.\"\"\"\n",
        "    keys = jr.split(jr.key(seed), num=batch_size)\n",
        "    return sample_func(keys, num_timesteps)[1][:, :, 0]\n",
        "\n",
        "\n",
        "# == Method to Train/Update Model Parameters == #\n",
        "# @partial(nnx.jit, static_argnums=(0, 1))\n",
        "@nnx.jit\n",
        "def train_step(model, optimizer, emissions):\n",
        "    \"\"\"Train for a single step.\"\"\"\n",
        "\n",
        "    # Create loss function for model `m` and emissions `e`\n",
        "    def loss_fn(m, e):\n",
        "        return optax.softmax_cross_entropy_with_integer_labels(\n",
        "            logits=m(e[:, :-1]), labels=e[:, 1:]\n",
        "        ).mean()\n",
        "\n",
        "    # Use jax to compute the gradient of the parameters\n",
        "    grad_fn = nnx.value_and_grad(loss_fn)\n",
        "\n",
        "    # Evaluate the loss and gradients.\n",
        "    loss, grads = grad_fn(model, emissions)\n",
        "\n",
        "    # Update parameters, return loss\n",
        "    optimizer.update(grads)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# == Test the Pytorch implementation to double-check the JAX implementation == #\n",
        "\n",
        "# pytorch_training_dataset = transformerlib.PytorchHMMDataset(\n",
        "#     emissions=torch.from_numpy(np.array(train_emissions)),\n",
        "#     states=torch.from_numpy(np.array(train_states)),\n",
        "# )\n",
        "# pytorch_dataset = torch.utils.data.DataLoader(\n",
        "#     pytorch_training_dataset, batch_size=batch_size\n",
        "# )\n",
        "# pytorch_model = transformerlib.PytorchTransformerLM(\n",
        "#     vocab_size=6,\n",
        "#     d_embedding=64,\n",
        "#     max_seq_len=512,\n",
        "#     num_layers=1,\n",
        "#     d_model=64,\n",
        "#     num_heads=2,\n",
        "#     d_feedforward=128,\n",
        "# )\n",
        "# if torch.cuda.is_available():\n",
        "#     pytorch_model = pytorch_model.cuda()\n",
        "\n",
        "# pytorch_optimizer = torch.optim.AdamW(\n",
        "#     pytorch_model.parameters(), lr=learning_rate, weight_decay=momentum\n",
        "# )\n",
        "# pytorch_loss_fn = torch.nn.CrossEntropyLoss()\n",
        "# pytorch_model.train()\n",
        "\n",
        "# pytorch_loss_history = []\n",
        "# num_iters_passed = 0\n",
        "# while num_iters_passed < 1000:\n",
        "#     for batch in pytorch_dataset:\n",
        "#         if num_iters_passed >= 1000:\n",
        "#             break\n",
        "#         emissions, states = batch\n",
        "#         emissions = emissions.long()\n",
        "#         if torch.cuda.is_available():\n",
        "#             emissions = emissions.cuda()\n",
        "#             states = states.cuda()\n",
        "\n",
        "#         cur_step = emissions[:, :-1]\n",
        "#         next_step = emissions[:, 1:]\n",
        "#         # For each time 't' through 'T-1', the transformer will learn to predict 't+1' given 0...'t'\n",
        "\n",
        "#         # Forward pass\n",
        "#         pytorch_optimizer.zero_grad()\n",
        "#         logits = pytorch_model(cur_step)\n",
        "#         loss = torch.nn.functional.cross_entropy(\n",
        "#             logits.reshape(-1, num_classes), next_step.reshape(-1)\n",
        "#         )\n",
        "\n",
        "#         # Backward pass\n",
        "#         loss.backward()\n",
        "#         pytorch_optimizer.step()\n",
        "#         pytorch_loss_history.append(loss.cpu().item())\n",
        "\n",
        "#         num_iters_passed += 1\n",
        "\n",
        "#         if num_iters_passed % 100 == 0:\n",
        "#             print(f\"PyTorch Iteration {num_iters_passed}: Loss {loss.cpu().item()}\")\n",
        "\n",
        "\n",
        "# pytorch_model.cpu().eval()\n",
        "# pytorch_test_em = torch.from_numpy(np.array(test_emissions[1][None, ...]))\n",
        "# pytorch_posterior = pytorch_model(pytorch_test_em[:, :-1])\n",
        "# pytorch_posterior = (\n",
        "#     torch.softmax(pytorch_posterior, dim=-1).detach().cpu().numpy().squeeze(0)\n",
        "# )\n",
        "\n",
        "# plt.figure()\n",
        "# plt.plot(pytorch_posterior[:, -1], label=\"P(6)\", color=\"red\")\n",
        "# plt.plot(pytorch_posterior[:, 0], label=\"P(1)\", color=\"blue\")\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ-d_RYu-xIC",
        "outputId": "3ecbe036-7dbc-4d62-ecc7-5067fe9c2918"
      },
      "outputs": [],
      "source": [
        "# == Train jax RNN and Transformer == #\n",
        "print(\"Training RNN\")\n",
        "recurrent_loss_history = []\n",
        "for i in trange(num_iterations):\n",
        "    idx = jr.randint(\n",
        "        jr.PRNGKey(i), shape=(batch_size,), minval=0, maxval=train_emissions.shape[0]\n",
        "    )\n",
        "    recurrent_loss_history.append(\n",
        "        train_step(recurrent_model, recurrent_optimizer, train_emissions[idx])\n",
        "    )\n",
        "\n",
        "print(\"Training Transformer\")\n",
        "transformer_loss_history = []\n",
        "for i in trange(num_iterations):\n",
        "    idx = jr.randint(\n",
        "        jr.PRNGKey(i), shape=(batch_size,), minval=0, maxval=train_emissions.shape[0]\n",
        "    )\n",
        "    transformer_loss_history.append(\n",
        "        train_step(transformer_model, transformer_optimizer, train_emissions[idx])\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# == Save models == #\n",
        "save_dir = Path(\"checkpoints\").absolute()\n",
        "if save_dir.exists():\n",
        "    # Remove existing checkpoints\n",
        "    import shutil\n",
        "\n",
        "    shutil.rmtree(save_dir)\n",
        "save_dir.mkdir(parents=True)\n",
        "\n",
        "transformer_meta = {\n",
        "    \"vocab_size\": transformer_model.vocab_size,\n",
        "    \"d_embedding\": transformer_model.d_embedding,\n",
        "    \"max_seq_len\": transformer_model.max_seq_len,\n",
        "    \"num_layers\": transformer_model.num_layers,\n",
        "    \"d_model\": transformer_model.d_model,\n",
        "    \"num_heads\": transformer_model.num_heads,\n",
        "    \"d_feedforward\": transformer_model.d_feedforward,\n",
        "    \"attn_dropout_p\": transformer_model.attn_dropout_p,\n",
        "}\n",
        "_, _, transformer_state = nnx.split(transformer_model, nnx.RngState, ...)\n",
        "transformer_ckptr = ocp.Checkpointer(ocp.CompositeCheckpointHandler())\n",
        "transformer_ckptr.save(\n",
        "    save_dir / f\"transformer_model_{num_train_samples}_samples\",\n",
        "    args=ocp.args.Composite(\n",
        "        state=ocp.args.StandardSave(transformer_state),\n",
        "        metadata=ocp.args.JsonSave(transformer_meta),\n",
        "    ),\n",
        ")\n",
        "\n",
        "recurrent_meta = {\n",
        "    \"dim_in\": recurrent_model.dim_in,\n",
        "    \"dim_hidden\": recurrent_model.dim_hidden,\n",
        "    \"dim_out\": recurrent_model.dim_out,\n",
        "    \"mlp_depth\": recurrent_model.mlp_depth,\n",
        "    \"num_layers\": recurrent_model.num_layers,\n",
        "}\n",
        "\n",
        "_, _, recurrent_state = nnx.split(recurrent_model, nnx.RngState, ...)\n",
        "recurrent_ckptr = ocp.Checkpointer(ocp.CompositeCheckpointHandler())\n",
        "recurrent_ckptr.save(\n",
        "    save_dir / f\"recurrent_model_{num_train_samples}_samples\",\n",
        "    args=ocp.args.Composite(\n",
        "        state=ocp.args.StandardSave(recurrent_state),\n",
        "        metadata=ocp.args.JsonSave(recurrent_meta),\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trial = 1\n",
        "transformer_predicted_probs = jax.nn.softmax(\n",
        "    transformer_model(test_emissions[trial][None, :-1]), axis=-1\n",
        ").squeeze(0)\n",
        "plt.plot(transformer_predicted_probs[:, -1], label=\"P(6)\", color=\"red\")\n",
        "plt.plot(transformer_predicted_probs[:, 0], label=\"P(1)\", color=\"blue\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "3v1HliLR8kbz",
        "outputId": "e1cac6c9-d3f1-4778-d92b-0cc4e9421226"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.plot(recurrent_loss_history, label=\"RNN\")\n",
        "ax.plot(transformer_loss_history, label=\"Transformer\")\n",
        "ax.set_yscale(\"log\")\n",
        "ax.set_xlabel(\"iteration\")\n",
        "ax.set_ylabel(\"loss\")\n",
        "ax.legend()\n",
        "ax.set_title(\"Training Loss\")\n",
        "ax.set_xlim(00, num_iterations)\n",
        "ax.set_ylim(min(recurrent_loss_history), recurrent_loss_history[100])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "-4x7_Ipe80PH",
        "outputId": "9ce55b6b-0549-4eff-e0f3-4c85cb36256b"
      },
      "outputs": [],
      "source": [
        "trial = 11\n",
        "posterior = hmm.filter(hmm_params, test_emissions[trial])\n",
        "transformer_predicted_probs = jax.nn.softmax(\n",
        "    transformer_model(test_emissions[trial][None, ...]), axis=-1\n",
        ").squeeze(0)\n",
        "rnn_predicted_probs = jax.nn.softmax(\n",
        "    recurrent_model(test_emissions[trial][None, ...]), axis=-1\n",
        ").squeeze(0)\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(\n",
        "    test_states[trial][None, :],\n",
        "    extent=(0, num_timesteps, 0, 1),\n",
        "    interpolation=\"none\",\n",
        "    aspect=\"auto\",\n",
        "    cmap=\"Greys\",\n",
        "    alpha=0.25,\n",
        ")\n",
        "plt.plot(\n",
        "    transformer_predicted_probs[:, -1],  # Predicted probability of '6' at each timestep\n",
        "    label=\"transformer prediction\",\n",
        "    color=\"red\",\n",
        ")\n",
        "plt.plot(\n",
        "    rnn_predicted_probs[:, -1],  # Predicted probability of '6' at each timestep\n",
        "    label=\"RNN prediction\",\n",
        "    color=\"blue\",\n",
        ")\n",
        "plt.plot(\n",
        "    posterior.filtered_probs @ hmm_params.emissions.probs[:, 0, -1],\n",
        "    label=\"Bayes optimal prediction\",\n",
        "    color=\"black\",\n",
        "    alpha=0.75,\n",
        ")\n",
        "plt.ylabel(\"probability\")\n",
        "plt.xlabel(\"time\")\n",
        "plt.title(\"Probability of a '6'\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Create a custom diverging color scaler\n",
        "mapper = plt.cm.ScalarMappable(\n",
        "    cmap=\"RdBu\", norm=mpl.colors.TwoSlopeNorm(vcenter=1 / 6, vmin=0, vmax=1)\n",
        ")\n",
        "\n",
        "gs = mpl.gridspec.GridSpec(\n",
        "    2, 2, width_ratios=[10, 1], height_ratios=[1, 1], wspace=0.05, hspace=0.25\n",
        ")\n",
        "fig = plt.figure(figsize=(8, 6))\n",
        "\n",
        "top = fig.add_subplot(gs[0, 0])  # Transformer plot\n",
        "bottom = fig.add_subplot(gs[1, 0])  # RNN plot\n",
        "cax = fig.add_subplot(gs[:, 1])  # colorbar\n",
        "\n",
        "fig.suptitle(\"$P(x_{t+1} | x_{0:t})$\")\n",
        "top.set_title(\"Transformer\")\n",
        "transformer_image = mapper.to_rgba(transformer_predicted_probs.T)\n",
        "im = top.imshow(\n",
        "    transformer_image,\n",
        "    aspect=\"auto\",\n",
        "    interpolation=\"none\",\n",
        "    extent=(0, num_timesteps, 6, 0),\n",
        ")\n",
        "top.set_ylabel(\"Outcome\")\n",
        "top.set_yticks(np.arange(0, 6) + 0.5)\n",
        "top.set_yticklabels([1, 2, 3, 4, 5, 6])\n",
        "\n",
        "bottom.set_title(\"RNN\")\n",
        "rnn_image = mapper.to_rgba(rnn_predicted_probs.T)\n",
        "im = bottom.imshow(\n",
        "    rnn_image,\n",
        "    aspect=\"auto\",\n",
        "    interpolation=\"none\",\n",
        "    extent=(0, num_timesteps, 6, 0),\n",
        ")\n",
        "bottom.set_xlabel(\"Time\")\n",
        "bottom.set_ylabel(\"Outcome\")\n",
        "bottom.set_yticks(np.arange(0, 6) + 0.5)\n",
        "bottom.set_yticklabels([1, 2, 3, 4, 5, 6])\n",
        "\n",
        "# Add colorbar\n",
        "cbar = fig.colorbar(mapper, cax=cax)\n",
        "cbar.set_label(\"Posterior probability\")\n",
        "cbar.set_ticks([0, 0.1, 1 / 6, 1 / 2, 1])\n",
        "cbar.set_ticklabels([\"0\", \"1/10\", \"1/6\", \"1/2\", \"1\"])\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "general",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
