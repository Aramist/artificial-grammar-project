"""A script to train transformers and RNNs on a batch of data generated by an HMM

This is designed to be standalone, but can be parallelized across a cluster using
batch_trainer.py
Args:
    --model: The model to train. Options are 'transformer' or 'rnn'.
    --train-size: The number of training examples to generate.
    --seq-len: The length of each sequence to generate.
    --hmm-params-file: Path to a NumPy npz file containing matrices 'transition_probs',
    'emission_probs', and 'initial_probs' describing the HMM.
    --output-dir: The directory to save the trained model.
    --num-iters: The number of epochs to train for.
"""

import argparse
from functools import partial
from pathlib import Path

import jax
import jax.numpy as jnp
import numpy as np
import optax
import utils
from flax import nnx
from rnnlib import LRUModel
from tqdm import trange
from transformerlib import TransformerLM


def make_transformer() -> tuple[TransformerLM, nnx.Optimizer]:
    transformer_model = TransformerLM(
        vocab_size=6,
        d_embedding=64,
        max_seq_len=512,
        num_layers=1,
        d_model=64,
        num_heads=1,
        d_feedforward=128,
        attn_dropout_p=0.0,
        rngs=nnx.Rngs(params=jax.random.key(0), dropout=jax.random.key(1)),
    )

    learning_rate = 0.01
    momentum = 0.9
    transformer_optimizer = nnx.Optimizer(
        transformer_model, optax.adamw(learning_rate, momentum)
    )

    return transformer_model, transformer_optimizer


def make_rnn() -> tuple[LRUModel, nnx.Optimizer]:
    recurrent_model = LRUModel(
        dim_in=6, dim_hidden=64, dim_out=6, mlp_depth=3, num_layers=3, rngs=nnx.Rngs(0)
    )

    learning_rate = 0.005
    momentum = 0.9
    recurrent_optimizer = nnx.Optimizer(
        recurrent_model, optax.adamw(learning_rate, momentum)
    )

    return recurrent_model, recurrent_optimizer


def make_training_data(
    train_size: int,
    seq_len: int,
    *,
    hmm_transition_probs: jax.Array,
    hmm_emission_probs: jax.Array,
    hmm_initial_probs: jax.Array,
) -> tuple[jax.Array, jax.Array]:
    """Generate training data from a hidden Markov model (HMM) for training.

    Args:
        train_size (int): Number of training examples to generate.
        seq_len (int): Length of each sequence.
        hmm_transition_probs (jax.Array): Transition probabilities of the hidden state, P(x_t | x_{t-1}).
        hmm_emission_probs (jax.Array): Emission probabilities of the observed state, P(y_t | x_t).
        hmm_initial_probs (jax.Array): Initial probabilities of the hidden state, P(x_0).

    Returns:
        jax.Array: States
        jax.Array: Emissions
    """

    states, emissions, _ = utils.sample_rollouts(
        train_size,
        sequence_length=seq_len,
        initial_probs=hmm_initial_probs,
        transition_matrix=hmm_transition_probs,
        emission_matrix=hmm_emission_probs,
        rng=jax.random.key(0),
    )

    return states, emissions


def train(
    model: nnx.Module,
    optimizer: nnx.Optimizer,
    training_emissions: jax.Array,
    num_iters: int,
    batch_size: int,
) -> None:
    """Trains the model"""

    @nnx.jit
    def train_step(model, optimizer, emissions):
        """Train for a single step."""

        # Create loss function for model `m` and emissions `e`
        def loss_fn(model, emissions):
            return optax.softmax_cross_entropy_with_integer_labels(
                logits=model(emissions[:, :-1]), labels=emissions[:, 1:]
            ).mean()

        # Evaluate the loss and gradients.
        grads = nnx.grad(loss_fn)(model, emissions)

        # Update parameters, return loss
        optimizer.update(grads)

    key = jax.random.key(123)
    for i in trange(num_iters):
        # Sample a mini-batch of training data
        key, subkey = jax.random.split(key)
        idx = jax.random.randint(
            subkey,
            shape=(batch_size,),
            minval=0,
            maxval=training_emissions.shape[0],
        )
        train_step(model, optimizer, training_emissions[idx])


if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument(
        "--model",
        type=str,
        choices=["transformer", "rnn"],
        required=True,
        help="The model to train.",
    )
    ap.add_argument(
        "--train-size",
        type=int,
        required=True,
        help="The number of training examples to generate.",
    )
    ap.add_argument(
        "--seq-len",
        type=int,
        default=512,
        help="The length of each sequence to generate.",
    )
    ap.add_argument(
        "--hmm-params-file",
        type=Path,
        required=True,
        help="Path to a NumPy npz file containing matrices 'transition_probs', "
        "'emission_probs', and 'initial_probs' describing the HMM.",
    )
    ap.add_argument(
        "--output-dir",
        type=Path,
        required=True,
        help="The directory to save the trained model.",
    )
    ap.add_argument(
        "--num-iters",
        type=int,
        default=50000,
        help="The number of weight updates to perform.",
    )
    args = ap.parse_args()

    # Ensure we don't waste time overwriting an existing model
    model_dir = args.output_dir
    if model_dir.exists():
        raise ValueError(f"Model directory {model_dir} already exists.")

    # Load the HMM parameters
    hmm_npz = np.load(args.hmm_params_file)
    hmm_transition_probs = jnp.array(hmm_npz["transition_probs"])
    hmm_emission_probs = jnp.array(hmm_npz["emission_probs"])
    hmm_initial_probs = jnp.array(hmm_npz["initial_probs"])

    # Generate training data
    _, training_emissions = make_training_data(
        args.train_size,
        args.seq_len,
        hmm_transition_probs=hmm_transition_probs,
        hmm_emission_probs=hmm_emission_probs,
        hmm_initial_probs=hmm_initial_probs,
    )

    # Create the model and optimizer
    if args.model == "transformer":
        model, optimizer = make_transformer()
    elif args.model == "rnn":
        model, optimizer = make_rnn()
    else:
        raise ValueError(f"Unknown model type: {args.model}")

    # Train the model
    loss_history = train(
        model,
        optimizer,
        training_emissions,
        num_iters=args.num_iters,
        batch_size=32,
    )

    # Save the model
    model.save(model_dir)
    print(f"Model saved to {model_dir}")
